{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54501a12-3b09-4e7c-8dee-94578bfc1798",
   "metadata": {},
   "source": [
    "### 1. Introduction and problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efa62b3-6147-4e86-943c-d7cf0366c4d5",
   "metadata": {},
   "source": [
    "#### a. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccc53e9-e110-4dfa-bd85-d544e2b7a74c",
   "metadata": {},
   "source": [
    "Reddit is an American social news aggregation, web content rating, and discussion website. Registered members submit content to the site such as links, text posts, images, and videos, which are then voted up or down by other members. Within each subreddit, there are text, image post, upvote or downvote posted by users to express their comments to the content of the post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d624c48-0e44-4a40-85e1-d266d061b211",
   "metadata": {},
   "source": [
    "#### b. Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d002629-113c-4257-9e22-97a3374d1440",
   "metadata": {},
   "source": [
    "For this project, we select the subreddit that similiar on surface `r/nottheonion` and `r/theonion`. Subreddit `r/nottheonion` focus on true stories that are so mind-blowingly ridiculous that you could have sworn they were from The Onion while subreddit `r/theonion` foucs American satirical articles on international, national, and local news.\n",
    "\n",
    "Due to the name of these 2 subreddits are highly appear to be misleading and confused. There will be problem for the moderator to maintain the subreddits. When performing maintenance, the moderator may accidentally deleted multiple posts from r/nottheonion and r/theonion. But, the moderator was not able to recover the titles of the lost posts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f867467-64b1-4533-83cb-74bb22f6e00b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Our goals of the project:\n",
    "\n",
    "Using the natural language processing to build a classification model for\n",
    "\n",
    "● training on posts submitted before 01 Jan 2022 to classify the \n",
    "recovered posts back to their respective subreddits, r/nottheonion \n",
    "and r/theonion, based solely on the post titles.\n",
    "\n",
    "● as a proof of concept for the development of an automated \n",
    "moderator which would automatically delete posts that do not \n",
    "belong to the subreddit that they are posted to.\n",
    "\n",
    "● Having automated moderators police the subreddit for spam posts \n",
    "would free up time for human moderators, who are volunteers, to do \n",
    "things that they want to do\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69800367-de8c-4124-a7e0-7550bbfb3577",
   "metadata": {},
   "source": [
    "### 2. Data Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaa47c0-caf9-47e5-86a3-72e9037d80dc",
   "metadata": {},
   "source": [
    "We are using PushShift API (PSAW) to extract upto maximum of 10,000 posts from each of these subreddits for the year 2021. For Reddit, each subreddit has it's own individual JSON that we can access. The most direct way to access the subreddit is through the Application Programming Interface (API) of the a site and extract a JavaScript Object notation (JSON). Then this JSON can be read into Python as a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8844432-53a6-4130-82f7-074a06b2772c",
   "metadata": {},
   "source": [
    "The information to be scrapped are followed:\n",
    "* Author of post\n",
    "* Domain (Permalink)\n",
    "* Title of post\n",
    "* Number of comments\n",
    "* Score\n",
    "* Date of post\n",
    "* Subscribers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83c75cb8-5dd8-4273-9571-28a1290e0bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API scrape \n",
    "from psaw import PushshiftAPI\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe90fb03-dc37-433a-8043-4856b60df31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the scrap function\n",
    "\n",
    "def scrap_data(subreddit):\n",
    "    \n",
    "    # Instantiate \n",
    "    api = PushshiftAPI()\n",
    "\n",
    "    # Create list of scraped data\n",
    "    scrape_list = list(api.search_submissions(subreddit=subreddit,\n",
    "                                after = 1609459200 ,before = 1640995200, # Specifying the cut off date as of 31 December 2021\n",
    "                                filter=['title', 'subreddit', 'num_comments', 'author', 'subreddit_subscribers', 'score', 'domain', 'created_utc'],\n",
    "                                limit=10000))\n",
    "\n",
    "    #Filter the list\n",
    "    clean_scrape_lst = []\n",
    "    for i in range(len(scrape_list)):\n",
    "        scrape_dict = {}\n",
    "        scrape_dict['subreddit'] = scrape_list[i][5]\n",
    "        scrape_dict['author'] = scrape_list[i][0]\n",
    "        scrape_dict['domain'] = scrape_list[i][2]\n",
    "        scrape_dict['title'] = scrape_list[i][7]\n",
    "        scrape_dict['num_comments'] = scrape_list[i][3]\n",
    "        scrape_dict['score'] = scrape_list[i][4]\n",
    "        scrape_dict['timestamp'] = scrape_list[i][1]\n",
    "        clean_scrape_lst.append(scrape_dict)\n",
    "\n",
    "    # Show number of subscribers\n",
    "    print(subreddit, 'subscribers:',scrape_list[1][6])\n",
    "    \n",
    "    # Return list of scraped data\n",
    "    return clean_scrape_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f233680-1b81-4dba-9069-63709e4e6d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nottheonion subscribers: 20438921\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame for nottheonion using the function\n",
    "\n",
    "df_not_onion = pd.DataFrame(scrap_data('nottheonion'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d5fad24-14a4-45b4-9fc7-d710410acc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to csv\n",
    "\n",
    "df_not_onion.to_csv('nottheonion.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36355dc0-52cf-46a3-b97f-4f6d6bc3b2d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9997, 7)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of DataFrame\n",
    "\n",
    "df_not_onion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce8bb2f9-e359-4057-ad27-54782332f2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nottheonion</td>\n",
       "      <td>Taco_duck68</td>\n",
       "      <td>wral.com</td>\n",
       "      <td>Man attempts to pay for car with rap, steals p...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1640995192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nottheonion</td>\n",
       "      <td>BlackNingaa</td>\n",
       "      <td>bloodyelbow.com</td>\n",
       "      <td>Former UFC fighter reveals past as sex worker ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1640994707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nottheonion</td>\n",
       "      <td>Lopsided_File_1642</td>\n",
       "      <td>facebook.com</td>\n",
       "      <td>Log into Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1640991506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nottheonion</td>\n",
       "      <td>SkinnyWhiteGirl19</td>\n",
       "      <td>theartnewspaper.com</td>\n",
       "      <td>McDonald’s blocked from building drive-through...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1640990429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nottheonion</td>\n",
       "      <td>kids-cake-and-crazy</td>\n",
       "      <td>kjrh.com</td>\n",
       "      <td>Legendary actress Betty White dies at 99 on Ne...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1640989181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     subreddit               author               domain  \\\n",
       "0  nottheonion          Taco_duck68             wral.com   \n",
       "1  nottheonion          BlackNingaa      bloodyelbow.com   \n",
       "2  nottheonion   Lopsided_File_1642         facebook.com   \n",
       "3  nottheonion    SkinnyWhiteGirl19  theartnewspaper.com   \n",
       "4  nottheonion  kids-cake-and-crazy             kjrh.com   \n",
       "\n",
       "                                               title  num_comments  score  \\\n",
       "0  Man attempts to pay for car with rap, steals p...             0      1   \n",
       "1  Former UFC fighter reveals past as sex worker ...             1      1   \n",
       "2                                  Log into Facebook             1      1   \n",
       "3  McDonald’s blocked from building drive-through...             0      1   \n",
       "4  Legendary actress Betty White dies at 99 on Ne...             0      1   \n",
       "\n",
       "    timestamp  \n",
       "0  1640995192  \n",
       "1  1640994707  \n",
       "2  1640991506  \n",
       "3  1640990429  \n",
       "4  1640989181  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show head\n",
    "\n",
    "df_not_onion.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "018947e9-64e3-4b94-92ee-d80aeadb839e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theonion subscribers: 165298\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame for theonion using the function\n",
    "\n",
    "df_onion = pd.DataFrame(scrap_data('theonion'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e8bc3f1-1cb0-4fdf-8526-2f4a5394c467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to csv\n",
    "\n",
    "df_onion.to_csv('theonion.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7137f56-f171-4ae7-a603-ceee08347ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1343, 7)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of DataFrame\n",
    "\n",
    "df_onion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fb34f9e-7a4f-446c-a93c-95fef2ea974f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>mothershipq</td>\n",
       "      <td>theonion.com</td>\n",
       "      <td>Surgeon Kind Of Pissed Patient Seeing Her Defo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1640973300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>-ImYourHuckleberry-</td>\n",
       "      <td>theartnewspaper.com</td>\n",
       "      <td>McDonald’s blocked from building drive-through...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1640971771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>dwaxe</td>\n",
       "      <td>theonion.com</td>\n",
       "      <td>Gwyneth Paltrow Touts New Diamond-Encrusted Tr...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1640955671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>dwaxe</td>\n",
       "      <td>theonion.com</td>\n",
       "      <td>Artist Crafting Music Box Hopes It Delights At...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1640955669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>dwaxe</td>\n",
       "      <td>theonion.com</td>\n",
       "      <td>Homeowner Trying To Smoke Out Snakes Accidenta...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1640955668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit               author               domain  \\\n",
       "0  TheOnion          mothershipq         theonion.com   \n",
       "1  TheOnion  -ImYourHuckleberry-  theartnewspaper.com   \n",
       "2  TheOnion                dwaxe         theonion.com   \n",
       "3  TheOnion                dwaxe         theonion.com   \n",
       "4  TheOnion                dwaxe         theonion.com   \n",
       "\n",
       "                                               title  num_comments  score  \\\n",
       "0  Surgeon Kind Of Pissed Patient Seeing Her Defo...             0      1   \n",
       "1  McDonald’s blocked from building drive-through...             1      1   \n",
       "2  Gwyneth Paltrow Touts New Diamond-Encrusted Tr...             0      1   \n",
       "3  Artist Crafting Music Box Hopes It Delights At...             0      1   \n",
       "4  Homeowner Trying To Smoke Out Snakes Accidenta...             0      1   \n",
       "\n",
       "    timestamp  \n",
       "0  1640973300  \n",
       "1  1640971771  \n",
       "2  1640955671  \n",
       "3  1640955669  \n",
       "4  1640955668  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show head\n",
    "df_onion.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad55593-a7cd-4cb9-8620-e102600811ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
